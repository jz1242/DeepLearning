{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is provided for Deep Learning (CS 482/682) Homework 6 practice.\n",
    "# The network structure is a simplified U-net. You need to finish the last layers\n",
    "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
    "# Modified by Hongtao Wu on Oct 11, 2019 for Fall 2019 Machine Learning: Deep Learning HW6\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms \n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for adding the convolution layer\n",
    "def add_conv_stage(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True, useBN=False):\n",
    "    if useBN:\n",
    "        # Use batch normalization\n",
    "        return nn.Sequential(\n",
    "          nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "          nn.BatchNorm2d(dim_out),\n",
    "          nn.LeakyReLU(0.1),\n",
    "          nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "          nn.BatchNorm2d(dim_out),\n",
    "          nn.LeakyReLU(0.1)\n",
    "        )\n",
    "    else:\n",
    "        # No batch normalization\n",
    "        return nn.Sequential(\n",
    "          nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "# Upsampling\n",
    "def upsample(ch_coarse, ch_fine):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "# U-Net\n",
    "class unet(nn.Module):\n",
    "    def __init__(self, useBN=False):\n",
    "        super(unet, self).__init__()\n",
    "        # Downgrade stages\n",
    "        self.conv1 = add_conv_stage(1, 32, useBN=useBN)\n",
    "        self.conv2 = add_conv_stage(32, 64, useBN=useBN)\n",
    "        self.conv3 = add_conv_stage(64, 128, useBN=useBN)\n",
    "        self.conv4 = add_conv_stage(128, 256, useBN=useBN)\n",
    "        # Upgrade stages\n",
    "        self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n",
    "        self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n",
    "        self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n",
    "        # Maxpool\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        # Upsample layers\n",
    "        self.upsample43 = upsample(256, 128)\n",
    "        self.upsample32 = upsample(128,  64)\n",
    "        self.upsample21 = upsample(64 ,  32)\n",
    "        # weight initialization\n",
    "        # You can have your own weight intialization. This is just an example.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "        #TODO: Design your last layer & activations\n",
    "#         self.convLast = add_conv_stage(32, 8, useBN=useBN, kernel_size=1, padding=0)\n",
    "        self.convLast = nn.Conv2d(32, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv2_out = self.conv2(self.max_pool(conv1_out))\n",
    "        conv3_out = self.conv3(self.max_pool(conv2_out))\n",
    "        conv4_out = self.conv4(self.max_pool(conv3_out))\n",
    "\n",
    "        conv4m_out_ = torch.cat((self.upsample43(conv4_out), conv3_out), 1)\n",
    "        conv3m_out  = self.conv3m(conv4m_out_)\n",
    "\n",
    "        conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n",
    "        conv2m_out  = self.conv2m(conv3m_out_)\n",
    "\n",
    "        conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n",
    "        conv1m_out  = self.conv1m(conv2m_out_)\n",
    "\n",
    "        #TODO: Design your last layer & activations\n",
    "        convfinal_out = self.convLast(conv1m_out)\n",
    "\n",
    "        return torch.softmax(convfinal_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "\n",
      "EPOCH 1 of 20\n",
      "\n",
      "tensor(12025.0566, device='cuda:0')\n",
      "\n",
      "Start Validation...\n",
      "tensor(10672.5254, device='cuda:0')\n",
      "tensor(11425.9062, device='cuda:0')\n",
      "tensor(11024.6084, device='cuda:0')\n",
      "tensor(10872.1689, device='cuda:0')\n",
      "tensor(10790.8223, device='cuda:0')\n",
      "\n",
      "EPOCH 2 of 20\n",
      "\n",
      "tensor(10794.3252, device='cuda:0')\n",
      "\n",
      "Start Validation...\n",
      "tensor(10774.1621, device='cuda:0')\n",
      "tensor(10810.3398, device='cuda:0')\n",
      "tensor(11061.5449, device='cuda:0')\n",
      "tensor(10747.0918, device='cuda:0')\n",
      "tensor(11392.8936, device='cuda:0')\n",
      "\n",
      "EPOCH 3 of 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################## Hyperparameters #################################\n",
    "# Batch size can be changed if it does not match your memory, please state your batch step_size\n",
    "# in your report.\n",
    "train_batch_size = 10\n",
    "validation_batch_size=10\n",
    "# Please use this learning rate for Prob1(a) and Prob1(b)\n",
    "learning_rate = 0.1\n",
    "# This num_epochs is designed for running to be long enough, you need to manually stop or design\n",
    "# your early stopping method.\n",
    "num_epochs = 20\n",
    "\n",
    "# TODO: Design your own dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input_dir):\n",
    "        datalist = []\n",
    "        labellist = []\n",
    "        onehotlabellist = []\n",
    "        for folders in os.listdir(input_dir):\n",
    "            new_dir = os.path.join(input_dir, folders)\n",
    "            if os.path.isdir(new_dir):\n",
    "                impath = new_dir+ '/'+folders+'_gray.jpg'\n",
    "                labelpath = new_dir+ '/'+folders+'_input.jpg'\n",
    "                im = Image.open(impath)\n",
    "                imLab = Image.open(labelpath)\n",
    "                im = np.array(im)\n",
    "                im = np.expand_dims(im, axis = 2)\n",
    "                datalist.append(im)\n",
    "                labellist.append(np.array(imLab))\n",
    "                \n",
    "        self.data = torch.from_numpy(np.transpose(np.array(datalist), (0, 3, 1, 2))).type('torch.FloatTensor')\n",
    "        self.label = torch.from_numpy(np.transpose(np.array(labellist), (0, 3, 1, 2))).type('torch.FloatTensor')\n",
    "        \n",
    "        \n",
    "    def mask(self, label, val):\n",
    "        return label & val == val\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# TODO: Use your designed dataset for dataloading\n",
    "train_dataset=ImageDataset(input_dir = \"./HW6_data/colorization/train_cor/\")\n",
    "validation_dataset=ImageDataset(input_dir =  \"./HW6_data/colorization/validation_cor/\")\n",
    "model = unet(useBN = True)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=train_batch_size, \n",
    "                                       shuffle=True)\n",
    "    \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                           batch_size=validation_batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "training_iter = iter(train_loader)\n",
    "validation_iter = iter(validation_loader)\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    ########################### Training #####################################\n",
    "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "    # TODO: Design your own training section\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    for ind, training_data in enumerate(train_loader):\n",
    "        x = autograd.Variable(training_data[0]).cuda()\n",
    "        y = autograd.Variable(training_data[1]).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        train_loss = loss(y_hat, y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss.append(train_loss.data)\n",
    "    print(train_loss.data)\n",
    "\n",
    "    ########################### Validation #####################################\n",
    "    # TODO: Design your own validation section\n",
    "    print(\"\\nStart Validation...\")\n",
    "    torch.set_grad_enabled(False)\n",
    "    for ind, valid_data in enumerate(validation_loader):\n",
    "        x = autograd.Variable(valid_data[0]).cuda()\n",
    "        y = autograd.Variable(valid_data[1]).cuda()\n",
    "        y_hat = model(x)\n",
    "        val_loss = loss(y_hat, y)\n",
    "        print(val_loss.data)\n",
    "        validation_loss.append(val_loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss, validation loss\n",
    "def plot(training_loss, validation_loss):\n",
    "    plt.figure(300)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')   \n",
    "    plt.plot(training_loss, 'b')\n",
    "    plt.figure(400)\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')   \n",
    "    plt.plot(validation_loss, 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(training_loss, validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
